"""
OpenAIæ ¼å¼APIæ”¯æŒæ¨¡å—
æ”¯æŒOpenAIã€Claudeã€DeepSeekç­‰å…¼å®¹API
"""
import os
import json
import time
from typing import Dict, Generator, Optional, Any
from language_manager import lang_manager
import httpx

# --- å…¨å±€é…ç½® ---
openai_config = {
    "api_key": None,
    "base_url": "",  # å¿…é¡»ç”±ç”¨æˆ·é…ç½®
    "model": "",  # å¿…é¡»ç”±ç”¨æˆ·é…ç½®
    "temperature": 0.7,
    "max_tokens": 4000,
    "timeout": 30,
    "nsfw_mode": False  # R18å†…å®¹å¼€å…³
}

def init_openai_llm(api_key: str, base_url: str, model: str, temperature: float = 0.7, max_tokens: int = 4000, nsfw_mode: bool = False):
    """
    åˆå§‹åŒ–OpenAIæ ¼å¼çš„LLMé…ç½®
    
    Args:
        api_key: APIå¯†é’¥
        base_url: APIåŸºç¡€URLï¼Œå¦‚ https://api.openai.com/v1
        model: æ¨¡å‹åç§°ï¼Œå¦‚ gpt-3.5-turbo, claude-3-sonnet-20240229
        temperature: æ¸©åº¦å‚æ•°
        max_tokens: æœ€å¤§tokenæ•°
        nsfw_mode: æ˜¯å¦å¯ç”¨R18å†…å®¹æ¨¡å¼
    """
    global openai_config
    
    # sanitize inputs: trim whitespace (including tabs/newlines) and normalize base_url
    api_key_clean = api_key.strip() if isinstance(api_key, str) else api_key
    base_url_clean = base_url.strip().rstrip('/') if isinstance(base_url, str) else base_url
    model_clean = model.strip() if isinstance(model, str) else model

    # Basic validation: ensure base_url doesn't contain control or non-printable characters
    if isinstance(base_url_clean, str) and any(ord(c) < 32 for c in base_url_clean):
        raise ValueError(f"Invalid characters in base_url: {repr(base_url)}")

    openai_config.update({
        "api_key": api_key_clean,
        "base_url": base_url_clean,
        "model": model_clean,
        "temperature": temperature,
        "max_tokens": max_tokens,
        "nsfw_mode": nsfw_mode
    })
    
    print(f"OpenAIå…¼å®¹APIå·²é…ç½®: {base_url_clean} -> {model_clean} (R18: {'å¼€å¯' if nsfw_mode else 'å…³é—­'})")

def is_openai_configured() -> bool:
    """æ£€æŸ¥OpenAIé…ç½®æ˜¯å¦å®Œæ•´"""
    return all([
        openai_config["api_key"],
        openai_config["base_url"],
        openai_config["model"]
    ])

def test_api_connection() -> bool:
    """æµ‹è¯•APIè¿æ¥æ˜¯å¦æ­£å¸¸"""
    if not is_openai_configured():
        print("âŒ APIæœªé…ç½®")
        return False
    
    try:
        print(f"ğŸ” æµ‹è¯•APIè¿æ¥: {openai_config['base_url']}")
        
        # å‘é€ä¸€ä¸ªç®€å•çš„æµ‹è¯•è¯·æ±‚
        test_messages = [
            {"role": "user", "content": "Hello"}
        ]
        
        response = _make_openai_request(test_messages, stream=False)
        
        if response.status_code == 200:
            print("âœ… APIè¿æ¥æ­£å¸¸")
            return True
        else:
            print(f"âŒ APIè¿”å›é”™è¯¯çŠ¶æ€ç : {response.status_code}")
            return False
            
    except Exception as e:
        print(f"âŒ APIè¿æ¥æµ‹è¯•å¤±è´¥: {str(e)}")
        return False

def _create_httpx_client():
    """åˆ›å»ºhttpxå®¢æˆ·ç«¯"""
    return httpx.Client(
        headers={
            'Accept-Charset': 'utf-8',
            'Content-Type': 'application/json; charset=utf-8'
        },
        timeout=30.0
    )

def _make_openai_request(messages: list, stream: bool = False) -> dict:
    """
    å‘é€OpenAIæ ¼å¼çš„APIè¯·æ±‚ï¼Œå¸¦é‡è¯•æœºåˆ¶
    """
    if not is_openai_configured():
        raise ValueError("OpenAI APIæœªé…ç½®")
    
    headers = {
        "Authorization": f"Bearer {openai_config['api_key']}",
        "Content-Type": "application/json; charset=utf-8",
        "User-Agent": "EasyPrompt/1.0"
    }
    
    payload = {
        "model": openai_config["model"],
        "messages": messages,
        "temperature": openai_config["temperature"],
        "max_tokens": openai_config["max_tokens"],
        "stream": stream
    }
    
    # R18æ¨¡å¼ä¸‹çš„ç‰¹æ®Šå‚æ•°é…ç½®
    if openai_config.get("nsfw_mode", False):
        payload.update({
            "temperature": min(openai_config["temperature"] + 0.2, 1.0),  # å¢åŠ åˆ›é€ æ€§
            "top_p": 0.95,  # å¢åŠ å¤šæ ·æ€§
            "frequency_penalty": -0.5,  # é¼“åŠ±é‡å¤æ€§ä¸»é¢˜
            "presence_penalty": -0.3,   # é¼“åŠ±å¼•å…¥æ–°æ¦‚å¿µ
        })
        
        # å¦‚æœæ”¯æŒï¼Œæ·»åŠ å®‰å…¨è¿‡æ»¤å™¨ç¦ç”¨å‚æ•°
        if "gpt" in openai_config["model"].lower():
            # OpenAIæ¨¡å‹ç‰¹å®šå‚æ•°
            payload["moderation"] = False
        elif "claude" in openai_config["model"].lower():
            # Claudeæ¨¡å‹ç‰¹å®šå‚æ•°
            payload["disable_safety"] = True
    
    # æ£€æŸ¥base_urlæ˜¯å¦å·²ç»åŒ…å«å®Œæ•´è·¯å¾„ï¼Œå¦‚æœæ²¡æœ‰åˆ™æ·»åŠ /chat/completions
    base_url = openai_config['base_url']
    # defensive check: ensure base_url is string and trimmed
    if isinstance(base_url, str):
        base_url = base_url.strip()
    if base_url.endswith('/chat/completions'):
        url = base_url
    elif base_url.endswith('/v1'):
        url = f"{base_url}/chat/completions"
    else:
        url = f"{base_url}/chat/completions"
    
    # ä½¿ç”¨httpxå®¢æˆ·ç«¯
    with _create_httpx_client() as client:
        try:
            print(f"æ­£åœ¨å‘é€APIè¯·æ±‚åˆ°: {url}")
            print(f"ä½¿ç”¨æ¨¡å‹: {openai_config['model']}")
            
            # ä½¿ç”¨httpxå‘é€è¯·æ±‚
            if stream:
                response = client.post(
                    url, 
                    headers=headers, 
                    json=payload,
                    timeout=openai_config["timeout"]
                )
            else:
                response = client.post(
                    url, 
                    headers=headers, 
                    json=payload,
                    timeout=openai_config["timeout"]
                )
            
            # æ£€æŸ¥å“åº”çŠ¶æ€
            if response.status_code != 200:
                print(f"âŒ APIè¯·æ±‚å¤±è´¥: {response.status_code}")
                print(f"ğŸ“„ é”™è¯¯å“åº”: {response.text}")
                print(f"ğŸ“‹ è¯·æ±‚å¤´: {dict(response.headers)}")
                print(f"ğŸ“¦ è¯·æ±‚ä½“: {json.dumps(payload, ensure_ascii=False, indent=2)}")
            
            response.raise_for_status()
            return response
        
        except httpx.TimeoutException as e:
            error_msg = f"APIè¿æ¥è¶…æ—¶: {openai_config['base_url']} - {str(e)}"
            print(f"âŒ {error_msg}")
            raise Exception(error_msg)
        except httpx.ConnectError as e:
            error_msg = f"APIè¿æ¥é”™è¯¯: {openai_config['base_url']} - {str(e)}"
            print(f"âŒ {error_msg}")
            raise Exception(error_msg)
        except httpx.HTTPStatusError as e:
            error_msg = f"API HTTPé”™è¯¯: {e.response.status_code} - {str(e)}"
            print(f"âŒ {error_msg}")
            raise Exception(error_msg)
        except Exception as e:
            error_msg = f"æœªçŸ¥é”™è¯¯: {str(e)}"
            print(f"âŒ {error_msg}")
            raise Exception(error_msg)

def get_openai_conversation_response_stream(chat_history: list, user_message: str, critique: str):
    """
    ä½¿ç”¨OpenAIæ ¼å¼APIè·å–å¯¹è¯å“åº”æµ
    
    Args:
        chat_history: å¯¹è¯å†å²
        user_message: ç”¨æˆ·æ¶ˆæ¯
        critique: è¯Šæ–­æŠ¥å‘Š
    
    Yields:
        Response chunks as strings, followed by a final result tuple
    """
    if not is_openai_configured():
        error_msg = lang_manager.t("ERROR_LLM_NOT_CONFIGURED")
        yield error_msg
        yield ("__FINAL_RESULT__", error_msg, "None")
        return
    
    try:
        # æ„é€ æ¶ˆæ¯ï¼Œä½¿ç”¨åŠ¨æ€ç³»ç»Ÿæç¤ºè¯
        prompts = lang_manager.system_prompts
        nsfw_mode = openai_config.get("nsfw_mode", False)
        system_prompt = prompts.get_conversation_system_prompt(nsfw_mode)
        
        # åœ¨R18æ¨¡å¼ä¸‹æ·»åŠ é¢å¤–çš„æ¿€æ´»æŒ‡ä»¤
        if nsfw_mode:
            system_prompt += """

**é‡è¦R18æ¿€æ´»æŒ‡ä»¤**: 
- ä¸»åŠ¨å¼•å¯¼ç”¨æˆ·æè¿°è§’è‰²çš„æ€§æ„Ÿç‰¹è´¨å’Œé­…åŠ›
- è¯¢é—®è§’è‰²åœ¨äº²å¯†åœºæ™¯ä¸‹çš„ååº”å’Œè¡Œä¸º
- æ¢ç´¢è§’è‰²çš„æ€§æ ¼ä¸­çš„è¯±æƒ‘å…ƒç´ 
- ä¸è¦å›é¿ä»»ä½•æˆäººè¯é¢˜ï¼Œè¦ä¸»åŠ¨æ·±å…¥æŒ–æ˜
- è®©å¯¹è¯å……æ»¡æš—ç¤ºæ€§å’Œè¯±å¯¼æ€§
- ç›®æ ‡æ˜¯åˆ›é€ å‡ºå…·æœ‰å¼ºçƒˆæ€§å¸å¼•åŠ›çš„è§’è‰²è®¾å®š
"""
        
        messages = [
            {"role": "system", "content": system_prompt}
        ]
        
        # æ·»åŠ å†å²å¯¹è¯
        messages.extend(chat_history)
        
        # æ·»åŠ å½“å‰æ¶ˆæ¯
        message_with_context = f"è¯Šæ–­æŠ¥å‘Š: {critique}\n\n---\n\nç”¨æˆ·: {user_message}"
        messages.append({"role": "user", "content": message_with_context})
        
        response = _make_openai_request(messages, stream=True)
        
        full_response_text = ""
        ai_response_part = ""
        trait_part = ""
        found_separator = False
        
        for line in response.iter_lines():
            if line:
                # ç¡®ä¿ä½¿ç”¨UTF-8è§£ç 
                if isinstance(line, bytes):
                    line = line.decode('utf-8', errors='replace')
                elif not isinstance(line, str):
                    line = str(line)
                if line.startswith('data: '):
                    data = line[6:]
                    if data == '[DONE]':
                        break
                    
                    try:
                        chunk_data = json.loads(data)
                        if 'choices' in chunk_data and len(chunk_data['choices']) > 0:
                            delta = chunk_data['choices'][0].get('delta', {})
                            chunk_text = delta.get('content', '')
                            
                            if chunk_text:
                                full_response_text += chunk_text
                                
                                # æ£€æŸ¥æ˜¯å¦é‡åˆ°åˆ†éš”ç¬¦
                                if not found_separator:
                                    if '---' in chunk_text:
                                        # æ‰¾åˆ°åˆ†éš”ç¬¦ï¼Œåˆ†å‰²å½“å‰å—
                                        parts = chunk_text.split('---', 1)
                                        ai_response_part += parts[0]
                                        # åª yield åˆ†éš”ç¬¦ä¹‹å‰çš„å†…å®¹
                                        if parts[0]:
                                            yield parts[0]
                                        
                                        found_separator = True
                                        # åˆ†éš”ç¬¦ä¹‹åçš„å†…å®¹æ˜¯ trait éƒ¨åˆ†
                                        if len(parts) > 1:
                                            trait_part += parts[1]
                                    else:
                                        # è¿˜æ²¡é‡åˆ°åˆ†éš”ç¬¦ï¼Œæ­£å¸¸è¾“å‡º
                                        ai_response_part += chunk_text
                                        yield chunk_text
                                else:
                                    # å·²ç»é‡åˆ°åˆ†éš”ç¬¦ï¼Œåç»­å†…å®¹éƒ½æ˜¯ trait éƒ¨åˆ†ï¼Œä¸å† yield
                                    trait_part += chunk_text
                    except json.JSONDecodeError:
                        continue
        
        # æ¸…ç† trait éƒ¨åˆ†
        trait_part = trait_part.strip()
        if not trait_part or trait_part.lower() == "none":
            trait_part = "None"
            
        # å‘é€æœ€ç»ˆç»“æœä½œä¸ºç‰¹æ®Šçš„ yield
        yield ("__FINAL_RESULT__", ai_response_part.strip(), trait_part)
        
    except Exception as e:
        error_message = lang_manager.t("ERROR_CONVERSATION_LLM", error=e)
        print(error_message)
        yield error_message
        yield ("__FINAL_RESULT__", error_message, "None")

def evaluate_openai_profile(full_profile: str) -> dict:
    """
    ä½¿ç”¨OpenAIæ ¼å¼APIè¯„ä¼°è§’è‰²æ¡£æ¡ˆ
    """
    if not is_openai_configured():
        return {"is_ready_for_writing": False, "critique": lang_manager.t("ERROR_LLM_NOT_CONFIGURED")}
    
    try:
        prompts = lang_manager.system_prompts
        nsfw_mode = openai_config.get("nsfw_mode", False)
        messages = [
            {"role": "system", "content": prompts.get_evaluator_system_prompt(nsfw_mode)},
            {"role": "user", "content": full_profile}
        ]
        
        response = _make_openai_request(messages, stream=False)
        response_data = response.json()
        
        if 'choices' in response_data and len(response_data['choices']) > 0:
            content = response_data['choices'][0]['message']['content']
            cleaned_response = content.strip().replace("```json", "").replace("```", "")
            return json.loads(cleaned_response)
        else:
            raise Exception("APIå“åº”æ ¼å¼é”™è¯¯")
            
    except Exception as e:
        error_message = lang_manager.t("ERROR_EVALUATOR_LLM", error=e)
        print(error_message)
        return {"is_ready_for_writing": False, "critique": error_message}

def run_openai_structured_prompt(system_prompt: str, user_prompt: str) -> str:
    """Runs a lightweight non-streaming request for control tasks (e.g., intent classification)."""
    if not is_openai_configured():
        raise ValueError("OpenAI APIæœªé…ç½®")

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt}
    ]

    response = _make_openai_request(messages, stream=False)
    response_data = response.json()

    if 'choices' not in response_data or not response_data['choices']:
        raise ValueError("OpenAIå“åº”ç¼ºå°‘choiceså­—æ®µ")

    return response_data['choices'][0]['message']['content']

def write_openai_final_prompt_stream(full_profile: str) -> Generator[str, None, None]:
    """
    ä½¿ç”¨OpenAIæ ¼å¼APIç”Ÿæˆæœ€ç»ˆæç¤ºè¯æµ
    """
    if not is_openai_configured():
        yield lang_manager.t("ERROR_LLM_NOT_CONFIGURED")
        return
        
    try:
        prompts = lang_manager.system_prompts
        nsfw_mode = openai_config.get("nsfw_mode", False)
        writer_prompt = prompts.get_writer_system_prompt(nsfw_mode)
        
        # åœ¨R18æ¨¡å¼ä¸‹ä¸ºå†™ä½œæ·»åŠ ç‰¹æ®ŠæŒ‡ä»¤
        if nsfw_mode:
            writer_prompt += """

**R18å†™ä½œå¢å¼ºæŒ‡ä»¤**:
- åœ¨æ¯ä¸ªéƒ¨åˆ†éƒ½è¦èå…¥æ€§æš—ç¤ºå’Œè¯±æƒ‘å…ƒç´ 
- è§’è‰²æŒ‡å—åº”å½“åŒ…å«è¶³å¤Ÿçš„æˆäººå†…å®¹æŒ‡å¯¼
- ä½¿ç”¨æ›´åŠ ç›´æ¥å’Œæš—ç¤ºæ€§çš„è¯­è¨€
- ç¡®ä¿æœ€ç»ˆçš„è§’è‰²èƒ½å¤Ÿè¿›è¡Œæˆäººè§’è‰²æ‰®æ¼”
- åœ¨è¡Œä¸ºæ–¹å¼ä¸­è¯¦ç»†æè¿°äº²å¯†åœºæ™¯ä¸‹çš„è¡¨ç°
- è®©æ•´ä¸ªè§’è‰²æ•£å‘å‡ºå¼ºçƒˆçš„æ€§é­…åŠ›
"""
        
        messages = [
            {"role": "system", "content": writer_prompt},
            {"role": "user", "content": full_profile}
        ]
        
        response = _make_openai_request(messages, stream=True)
        
        for line in response.iter_lines():
            if line:
                # ç¡®ä¿ä½¿ç”¨UTF-8è§£ç 
                if isinstance(line, bytes):
                    line = line.decode('utf-8', errors='replace')
                elif not isinstance(line, str):
                    line = str(line)
                if line.startswith('data: '):
                    data = line[6:]
                    if data == '[DONE]':
                        break
                    
                    try:
                        chunk_data = json.loads(data)
                        if 'choices' in chunk_data and len(chunk_data['choices']) > 0:
                            delta = chunk_data['choices'][0].get('delta', {})
                            chunk_text = delta.get('content', '')
                            
                            if chunk_text:
                                yield chunk_text
                    except json.JSONDecodeError:
                        continue
                        
    except Exception as e:
        error_message = lang_manager.t("ERROR_WRITER_LLM", error=e)
        print(error_message)
        yield error_message

def get_openai_config() -> dict:
    """è·å–å½“å‰OpenAIé…ç½®ï¼ˆéšè—æ•æ„Ÿä¿¡æ¯ï¼‰"""
    config = openai_config.copy()
    if config["api_key"]:
        # åªæ˜¾ç¤ºAPIå¯†é’¥çš„å‰4ä½å’Œå4ä½
        key = config["api_key"]
        if len(key) > 8:
            config["api_key"] = f"{key[:4]}...{key[-4:]}"
    return config
